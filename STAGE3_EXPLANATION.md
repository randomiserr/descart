# Stage 3: LLM Explanation - Implementation Summary

## Overview
Stage 3 ensures the LLM **never invents numbers**. All calculations come from Stage 2 (Python), and the LLM's job is **purely synthesis and explanation**.

## Key Changes

### 1. **Strict No-Calculation Policy**
The system prompt now includes:
```
ğŸ”´ KRITICKÃ PRAVIDLA:
1. NIKDY nevymÃ½Å¡lejte ÄÃ­sla ani vÃ½poÄty
2. PouÅ¾Ã­vejte POUZE vÃ½sledky z sekce "MODELOVÃ‰ VÃPOÄŒTY"
3. Pokud vÃ½poÄet chybÃ­, napiÅ¡te: "PÅ™esnÃ½ fiskÃ¡lnÃ­ dopad nebylo moÅ¾nÃ© vypoÄÃ­tat."
```

### 2. **Temperature Lowered to 0.2**
```python
response = self.llm.generate_response(prompt, system_prompt, temperature=0.2)
```

This significantly reduces randomness and hallucinations.

### 3. **Calculation Results Prominently Displayed**
The prompt now includes:

```
MODELOVÃ‰ VÃPOÄŒTY (POUÅ½IJTE TYTO HODNOTY):

[Claim 1] "PÅ™idÃ¡me 5000 KÄ kaÅ¾dÃ©mu hasiÄi..."
  â†’ NÃKLAD: 57,500,000 CZK
  â†’ Formule: 11,500 (PoÄet hasiÄÅ¯) * 5,000 CZK
  â†’ PÅ™edpoklady: Applied to all: PoÄet hasiÄÅ¯ (HZS)
```

### 4. **Source Registry**
Instead of dumping raw data, the LLM receives:
- A **source registry** (mapping source_id â†’ human-readable name)
- A **concise context summary** (not full JSON dumps)

Example:
```
DOSTUPNÃ‰ ZDROJE:
- csu_pop_firefighters: HZS ÄŒR: PoÄet hasiÄÅ¯ (2023)
- budget_2025_chapter_307: StÃ¡tnÃ­ rozpoÄet 2025
- web_search_1: Diskuze o valorizaci dÅ¯chodÅ¯
```

### 5. **Minimal Context Injection**
The LLM no longer receives:
- âŒ Full budget JSON dumps
- âŒ Full legal documents
- âŒ Raw macro data

Instead, it gets:
- âœ… Summary counts ("202007 budget items, total 1.9T CZK")
- âœ… Source titles ("ZÃ¡kon o DPH, ZÃ¡kon o danÃ­ch z pÅ™Ã­jmÅ¯")
- âœ… Calculation results with exact formulas

## Data Flow (Stage 0 â†’ 3)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 0     â”‚  LLM (temp=0.2) extracts structured claims
â”‚  Extraction  â”‚  â†’ {"text": "...", "value_czk": 5000, "target": "hasiÄi"}
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1     â”‚  Python searches catalog for "hasiÄi"
â”‚  Retrieval   â”‚  â†’ Finds: {id: "pop_firefighters", value: 11500}
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2     â”‚  Python calculates: 5000 * 11500 = 57.5M CZK
â”‚  Calculation â”‚  â†’ CalculationResult(cost_czk=57500000, formula="...")
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3     â”‚  LLM (temp=0.2) writes:
â”‚  Explanation â”‚  "NavÃ½Å¡enÃ­ platÅ¯ hasiÄÅ¯ o 5000 KÄ mÄ›sÃ­ÄnÄ› bude
â”‚              â”‚   <cite source='ModelovÃ½ vÃ½poÄet'>stÃ¡t 57,5 mil. CZK
â”‚              â”‚   roÄnÄ›</cite> (11 500 hasiÄÅ¯ Ã— 5000 KÄ)."
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Testing

Run `test_stage3.py` to verify:
- âœ… Full pipeline (Stages 0-3) executes
- âœ… Calculation results are included in the prompt
- âœ… LLM uses `<cite>` tags
- âœ… LLM references 'ModelovÃ½ vÃ½poÄet'

## Benefits

1. **Determinism**: Same claim â†’ same calculation â†’ same number
2. **Transparency**: Formula is shown ("X * Y")
3. **Auditability**: Sources are tracked (source_id)
4. **Accuracy**: LLM can't make arithmetic errors
5. **Factuality**: Temperature 0.2 reduces hallucinations

## Comparison: Before vs After

### Before (Old System)
```
Prompt: "SpoÄÃ­tejte dopad zvÃ½Å¡enÃ­ platÅ¯ hasiÄÅ¯ o 5000 KÄ"
LLM: "Odhaduji, Å¾e to bude stÃ¡t asi 60-80 miliard KÄ roÄnÄ›..."
```
âŒ Wrong by 1000x! (hallucinates "miliard" instead of "milion")

### After (Stage 3)
```
Prompt: "NÃKLAD: 57,500,000 CZK (PoÄet hasiÄÅ¯: 11,500 Ã— 5,000 CZK)"
LLM: "NÃ¡klady jsou <cite source='ModelovÃ½ vÃ½poÄet'>57,5 mil. CZK</cite>"
```
âœ… Correct! LLM just reformats the pre-calculated number.

## Future Enhancements

1. **Strict Schema Validation**: Validate that the LLM didn't add any extra numbers
2. **Number Extraction Test**: Parse the report and verify all numbers came from Stage 2
3. **Citation Coverage**: Track which sources were actually cited vs. available
4. **A/B Testing**: Compare temperature 0.2 vs 0.0 vs 0.5 for quality

## Files Modified

- `orchestrator.py`:
  - `_synthesize_report()` - Refactored for Stage 3
  - `_format_calculations_for_prompt()` - New method
  - `_build_source_registry()` - New method
  - `_format_source_list()` - New method
  - `_format_context_for_prompt()` - New method

## Next Steps

You can now:
1. Test the full pipeline with `test_stage3.py`
2. Inspect the generated reports for citation quality
3. Add more formulas (Stage 2) to improve coverage
4. Review unsupported claims log to prioritize new formulas
